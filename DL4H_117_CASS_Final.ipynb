{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CS 598 Deep Learning for HealthCare : Project Report"
      ],
      "metadata": {
        "id": "ID3AmCPYHb74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchcontrib pytorch_lightning timm tensorboard torchmetrics gdown kaggle"
      ],
      "metadata": {
        "id": "MCw2oCrN_heZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Github URL\n",
        "\n",
        "https://github.com/ShineyAji/DLH_CASS"
      ],
      "metadata": {
        "id": "i4WHB8cLNQo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Presentation URL\n",
        "\n",
        "https://drive.google.com/file/d/1DLBOyd5vJFx-TnHBvq-T7f7Sfhgl_erD/view?usp=drive_link"
      ],
      "metadata": {
        "id": "o_6VM2xYEVIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "A2NeUFbmMqIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper we selected for this project is: Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision by Singh and Cirrone (2023).\n",
        "\n",
        "\n",
        "*   **Background:**\\\n",
        "    In healthcare and biomedical applications, the primary challenges revolve around the substantial computational demands and the limited availability of data.Representation learning has the potential to improve deep learning models by learning valuable priors from limited medical data. Despite their benefits, existing representation and self supervised methods are computationally intensive and requires multiple GPU servers operating for extended period, which restricts accessibility for general practitioner.\n",
        "    \n",
        "    Labelling medical data requires specialized domain knowledge and can be expensive and time consuming. Also releasing medical datasets for research is restricted by privacy and regulatory constraints. Moreover, there are limitations in understanding the diseases, because the disease is emerging or there is a lack of systematic data collection mechanisms. All these conditions leads to medical data scarcity, both labelled and unlabeled.Even though reducing pretraining epochs can mitigate the computational problem and small batch size can mitigate the data scarcity problem, the state of art self supervised techniques drop significant performance when used with small batch sizes and reduced epochs.\n",
        "\n",
        "*   **CASS:**\\\n",
        "    The paper proposes Cross Architectural - Self Supervision (CASS), which is a self supervised learning approach that combines CNN and Transformer in a response-based siamese contrastive method. Siamese cross-architecture techniques combine CNNs and Transformers without any change to their architecture to help both of them learn better representations. In this approach an image is passed through a common set of augmentations, and the augmented image simultaneously pass through a CNN and Transformer to create positive pairs. The output logits from CNN and Transformer are then used to find Cosine Similarity Loss. CASS learns more predictive data representations in limited data scenarios where a Transformer only model cannot find them.\n",
        "\n",
        "    The authors observed that self-supervised pre-training performs better than transfer learning in all cases by a margin. They also observed that CASS improves upon the classification performance of existing state-of-the-art self-supervised method. They concluded that with CASS, the researchers can begin medical image analysis, even with a small amount of the overall dataset or even if only a small portion is labeled.\n",
        "\n",
        "*   **Datasets:**\\\n",
        "    We are using 2 datasets in this project that was used by the authors in the paper: Brain Tumor MRI Classification dataset that consists of 7023 samples of black-and- white MRI images, and ISIC 2019 dataset that contains 25,331 images of skin lesion. \\\n",
        "\n",
        "    ********** **In this notebook, we will be using only Brain Tumor MRI Classification dataset.** All the data preprocessing were done in this notebook only for the Brain Tumor MRI dataset. We added another notebook (DLH_CASS_ISIC2009.ipynb) in github with the implementation code for dataset preprocessing and model training for ISIC 2019 dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data from Kaggle"
      ],
      "metadata": {
        "id": "WMa9zoUyBp20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the data from kaggle into colab\n",
        "!gdown --id '18WnN-JkA_hv_Q3TWtZcnuk6euc2YwZ3D' --output folder_structure.png #folder structure from kaggle\n",
        "!gdown --id '12rHOJgPKyvbM-sEL2IbQth1lAIXT6hdW' --output kaggle.json\n",
        "!rm -rf ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "! cp /content/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset\n",
        "!rm -rf BrainMRI\n",
        "!mkdir BrainMRI\n",
        "!unzip brain-tumor-mri-dataset.zip -d BrainMRI"
      ],
      "metadata": {
        "id": "iXy_rkGz2vHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: If we evaluate CASS and other self-supervised methods on the given datasets, CASS will show high performance over the existing self-supervised techniques\n",
        "2.   Hypothesis 2: If trained for reduced epochs or batch size, existing methods will show a performance drop whereas CASS will be robust to these changes.\n",
        "3.   Hypothesis 3: The training time of CASS will be lesser than the training time of other self-supervised methods. *With the limited computing resource, we won't be able to exactly produce the performance result as original paper. But we can test this hypothesis in relative term*\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CASS Implementation\n",
        "\n",
        "We will implement the code for CASS and train the model using Brain Tumor MRI dataset in this notebook. \\\n",
        "\n",
        "**Code Source:** We reproduced the source code for the model from the original paper (https://github.com/pranavsinghps1/CASS) with changes needed for our dataset. \\\n",
        "\n",
        "**Changes Incorporated:** Source repository from the paper authors has the code for MedMNIST and ISIS 2019 dataset. So the data preprocessing for BrainMRI dataset to fit into the designed model was completely done by us. Also we got error during loss calculation because of code issue from source, which we fixed.\n",
        "\n",
        "\n",
        "**Changes planned to implement :** Planning to implement necessary changes for ablation studies which will be captured in final report."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Dependencies"
      ],
      "metadata": {
        "id": "sDSFSkgyC9Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import pandas as pd\n",
        "import timm\n",
        "import math\n",
        "import gdown\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "from torchvision import transforms as tsfm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torchcontrib.optim import SWA\n",
        "from torchmetrics import Metric\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    label_num2str = {0: 'glioma',\n",
        "                     1: 'meningioma',\n",
        "                     2: 'notumor',\n",
        "                     3: 'pituitary'\n",
        "                     }\n",
        "    label_str2num = {'glioma': 0,\n",
        "                     'meningioma':1,\n",
        "                     'notumor':2,\n",
        "                     'pituitary':3\n",
        "                     }\n",
        "    fl_alpha = 1.0  # alpha of focal_loss\n",
        "    fl_gamma = 2.0  # gamma of focal_loss\n",
        "    cls_weight = [0.2, 0.5970802919708029, 1.0, 0.25255474452554744]\n",
        "    cnn_name='resnet50'\n",
        "    vit_name='vit_base_patch16_384'\n",
        "    seed = 77\n",
        "    num_classes = 4\n",
        "    batch_size = 16\n",
        "    t_max = 16\n",
        "    lr = 1e-3\n",
        "    min_lr = 1e-6\n",
        "    n_fold = 6\n",
        "    num_workers = 2 # In GCP terminal, we ran this with 8 workers\n",
        "    accum_grad_batch = 1\n",
        "    early_stop_delta = 1e-7\n",
        "    gpu_idx = 0\n",
        "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
        "    gpu_list = [gpu_idx]"
      ],
      "metadata": {
        "id": "7GCRDcMHy7iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * This dataset contains 7k+ samples of human brain MRI images which can be accessed from https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset.\n",
        "  * While training the model using GCP, we got the data from kaggle by running the API command in terminal: \\\n",
        "       **API Command:** *kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset*\n",
        "  * This datset has 7023 images of human brain MRI images classified into 4 classes: glioma, meningioma, no tumor and pituitary. The curator created predefined training and testing splits. 5712 images for training and 1311 images for testing.This collection of MRI images are combination of multiple datasets, the size of images varies throughout the dataset.\n",
        "  * Using 'label_str2num', we performed the class to integer mapping, by mapping each labels into one hot vector.\n",
        "\n",
        "This collection of MRI images are combination of multiple datasets, the size of images varies throughout the dataset. So we resized the images to uniform size.\n"
      ],
      "metadata": {
        "id": "sJ7Nh34RfkdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This image shows the structure of training and testing data. There are 2 folders, one each for training and testing and the subfolders has the class name for the images inside those folders.\n",
        "img = cv2.imread('/content/folder_structure.png')\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "nuiFj0drrult"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the training data from the path where the data downloaded from kaggle is saved.\n",
        "# The folder structure was shown above. Since each of these folders has more images and gdown allows only maximum 50 files per folder,\n",
        "#  we couldn't use gdown to get the files into colab notebook.\n",
        "\n",
        "train_data_dir = 'BrainMRI/Training'\n",
        "train_filepaths = []\n",
        "train_labels = []\n",
        "all_train_img_labels_ts = []\n",
        "\n",
        "folds = os.listdir(train_data_dir)\n",
        "for fold in folds:\n",
        "     if not fold.startswith('.'):\n",
        "        foldpath = os.path.join(train_data_dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            # perform the class to integer mapping, by mapping each labels into one hot vector\n",
        "            train_filepaths.append(fpath)\n",
        "            train_labels.append(fold)\n",
        "            tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
        "            label_num=CFG.label_str2num.get(fold)\n",
        "            k=int(label_num)\n",
        "            tmp_label[k] = 1.0\n",
        "            all_train_img_labels_ts.append(tmp_label)\n",
        "\n",
        "\n",
        "ts_label_list = [tensor.tolist() for tensor in all_train_img_labels_ts]\n",
        "# Concatenate data paths with labels into one dataframe\n",
        "Fseries = pd.Series(train_filepaths, name= 'filepaths')\n",
        "Lseries = pd.DataFrame({'labels': ts_label_list})\n",
        "train_df = pd.concat([Fseries, Lseries], axis= 1)"
      ],
      "metadata": {
        "id": "S5SfZTpQzJdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the testing data from the path where the data downloaded from kaggle is saved.\n",
        "# The folder structure was shown above. Since each of these folders has more images and gdown allows only maximum 50 files per folder,\n",
        "#  we couldn't use gdown to get the files into colab notebook.\n",
        "\n",
        "test_data_dir = 'BrainMRI/Testing'\n",
        "test_filepaths = []\n",
        "test_labels = []\n",
        "all_test_img_labels_ts = []\n",
        "\n",
        "folds = os.listdir(test_data_dir)\n",
        "for fold in folds:\n",
        "      if not fold.startswith('.'):\n",
        "        foldpath = os.path.join(test_data_dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            # perform the class to integer mapping, by mapping each labels into one hot vector\n",
        "            test_filepaths.append(fpath)\n",
        "            test_labels.append(fold)\n",
        "            tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
        "            label_num=CFG.label_str2num.get(fold)\n",
        "            k=int(label_num)\n",
        "            tmp_label[k] = 1.0\n",
        "            all_test_img_labels_ts.append(tmp_label)\n",
        "\n",
        "ts_label_list = [tensor.tolist() for tensor in all_test_img_labels_ts]\n",
        "Fseries = pd.Series(test_filepaths, name='filepaths')\n",
        "Lseries = pd.DataFrame({'labels': ts_label_list})\n",
        "ts_df = pd.concat([Fseries, Lseries], axis=1)"
      ],
      "metadata": {
        "id": "eQ9Fi8l3zNUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*************Training Dataset Statistics**************\")\n",
        "print(\"Num of images in train dataset:\",len(train_df))\n",
        "train_glioma_count = sum(1 for label in train_df['labels'] if label[0] == 1.0)\n",
        "train_meningioma_count = sum(1 for label in train_df['labels'] if label[1] == 1.0)\n",
        "train_notumor_count = sum(1 for label in train_df['labels'] if label[2] == 1.0)\n",
        "train_pituitary_count = sum(1 for label in train_df['labels'] if label[3] == 1.0)\n",
        "print(\"Total Number of images with glioma in training dataset:\", train_glioma_count)\n",
        "print(\"Total Number of images with meningioma in training dataset:\", train_meningioma_count)\n",
        "print(\"Total Number of images with notumor in training dataset:\", train_notumor_count)\n",
        "print(\"Total Number of images with pituitary in training dataset:\", train_pituitary_count)\n"
      ],
      "metadata": {
        "id": "j00km518hSww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_everything(77)\n",
        "cfg=CFG()"
      ],
      "metadata": {
        "id": "_C7d0Q3WzdGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we split the data from testing into two sets, one for validation and one for testing."
      ],
      "metadata": {
        "id": "RqbrZdMwxiM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df, test_df = train_test_split(ts_df,  train_size= 0.5, shuffle= True, random_state= 123)"
      ],
      "metadata": {
        "id": "bBXQj987zfHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*************Validation Dataset Statistics**************\")\n",
        "print(\"Num of images in test dataset:\",len(valid_df))\n",
        "valid_glioma_count = sum(1 for label in valid_df['labels'] if label[0] == 1.0)\n",
        "valid_meningioma_count = sum(1 for label in valid_df['labels'] if label[1] == 1.0)\n",
        "valid_notumor_count = sum(1 for label in valid_df['labels'] if label[2] == 1.0)\n",
        "valid_pituitary_count = sum(1 for label in valid_df['labels'] if label[3] == 1.0)\n",
        "print(\"Total Number of images with glioma in testing dataset:\", valid_glioma_count)\n",
        "print(\"Total Number of images with meningioma in testing dataset:\", valid_meningioma_count)\n",
        "print(\"Total Number of images with notumor in testing dataset:\", valid_notumor_count)\n",
        "print(\"Total Number of images with pituitary in testing dataset:\", valid_pituitary_count)\n",
        "\n",
        "print(\"*************Testing Dataset Statistics**************\")\n",
        "print(\"Num of images in test dataset:\",len(test_df))\n",
        "test_glioma_count = sum(1 for label in test_df['labels'] if label[0] == 1.0)\n",
        "test_meningioma_count = sum(1 for label in test_df['labels'] if label[1] == 1.0)\n",
        "test_notumor_count = sum(1 for label in test_df['labels'] if label[2] == 1.0)\n",
        "test_pituitary_count = sum(1 for label in test_df['labels'] if label[3] == 1.0)\n",
        "print(\"Total Number of images with glioma in testing dataset:\", test_glioma_count)\n",
        "print(\"Total Number of images with meningioma in testing dataset:\", test_meningioma_count)\n",
        "print(\"Total Number of images with notumor in testing dataset:\", test_notumor_count)\n",
        "print(\"Total Number of images with pituitary in testing dataset:\", test_pituitary_count)"
      ],
      "metadata": {
        "id": "fjD5ZNH_p5XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_list = train_filepaths\n",
        "train_label_list = all_train_img_labels_ts\n",
        "valid_image_list = valid_df['filepaths'].tolist()\n",
        "valid_label_list = valid_df['labels'].tolist()\n",
        "all_valid_label_list = [torch.tensor(sublist) for sublist in valid_label_list]\n",
        "test_image_list = test_df['filepaths'].tolist()\n",
        "test_label_list = test_df['labels'].tolist()\n",
        "all_test_label_list = [torch.tensor(sublist) for sublist in test_label_list]"
      ],
      "metadata": {
        "id": "LS3lXyUzzhxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define image transformation\n",
        "\"\"\"\n",
        "DATASET_IMAGE_MEAN = (0.485, 0.456, 0.406)\n",
        "DATASET_IMAGE_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
        "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomPerspective(distortion_scale=0.2),], p=0.3),\n",
        "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomAffine(degrees=10),], p=0.3),\n",
        "                                tsfm.RandomVerticalFlip(p=0.3),\n",
        "                                tsfm.RandomHorizontalFlip(p=0.3),\n",
        "                                tsfm.ToTensor(),\n",
        "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
        "\n",
        "valid_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
        "                                tsfm.ToTensor(),\n",
        "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
        "\n",
        "test_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
        "                                tsfm.ToTensor(),\n",
        "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])"
      ],
      "metadata": {
        "id": "ged6UN3cznvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define dataset class\n",
        "\"\"\"\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, cfg, image: list, labels: list, transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image = image\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = self.image[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_ts = self.transform(img)\n",
        "        label_ts = self.labels[idx]\n",
        "\n",
        "        return img_ts, label_ts"
      ],
      "metadata": {
        "id": "jUkDGnX3zrSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n",
        "CASS combines CNNs and Transformers without any change to their architecture to help both of them learn better representation. The architecture of CASS is below:\n",
        "\\\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1NSE7agnZrrTYpu-aNAzrhJ8SYIyM-oaN)\n",
        "\n",
        "In CASS, R represents ResNet50, a CNN and T in the other box represents the Transformer used (ViT - Vision Transformer); X is the input image, which becomes X’ after applying augmentations. The output logits from the CNN and Transformer are then used to find cosine similarity\n",
        "loss. CASS applies only one set of augmentations to create X’. X’ is passed through both arms to compute loss.\n",
        "![picture](https://drive.google.com/uc?/export=view&id=16U9D1v0kO0bixk3y07RXAWluq7oppI-A)\n",
        "\n",
        "R and T represent embeddings from CNN and Transformer, respectively. Same set of parameters for both architectures’ optimizer and learning schedule. This experiment used stochastic weigh averaging with Adam optimizer and a learning rate of 1e-3. For the learning rate, cosine schedule with a maximum of 16 iterations and a minimum value of 1e-6 was used."
      ],
      "metadata": {
        "id": "RsCx461c1WXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN and Transformer:** For CNN and Transformer implementation, Timm's library was being used for the pretrained models. Details in https://github.com/huggingface/pytorch-image-models"
      ],
      "metadata": {
        "id": "mbZWtVqIV84p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
        "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
        "model_cnn.to(device)\n",
        "model_vit.to(device)"
      ],
      "metadata": {
        "id": "K72prQAOz_Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Self-Supervised model with both CNN & Transformer(ViT)\n",
        "def ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs):\n",
        "    writer = SummaryWriter()\n",
        "    phase = 'train'\n",
        "    model_cnn.train()\n",
        "    model_vit.train()\n",
        "    f1_score_cnn=0\n",
        "    f1_score_vit=0\n",
        "    for i in tqdm(range(num_epochs)):\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            for img,_ in train_loader:\n",
        "                f1_score_cnn=0\n",
        "                f1_score_vit=0\n",
        "                img = img.to(device)\n",
        "                pred_vit = model_vit(img)\n",
        "                pred_cnn = model_cnn(img)\n",
        "                model_sim_loss=loss_fn(pred_vit,pred_cnn)\n",
        "                loss = model_sim_loss.mean()\n",
        "                loss.backward()\n",
        "                optimizer_cnn.step()\n",
        "                optimizer_vit.step()\n",
        "                scheduler_cnn.step()\n",
        "                scheduler_vit.step()\n",
        "            print('For -',i,'Loss:',loss)\n",
        "            writer.add_scalar(\"Self-Supervised Loss/train\", loss, i)\n",
        "    writer.flush()"
      ],
      "metadata": {
        "id": "2XVm_ZSQ0EDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the loss function for the calculation mentioned above\n",
        "def loss_fn(x, y):\n",
        "    x =  torch.nn.functional.normalize(x, dim=-1, p=2)\n",
        "    y =  torch.nn.functional.normalize(y, dim=-1, p=2)\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)"
      ],
      "metadata": {
        "id": "IJ1XfPjD0m9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Focal Loss:** Since all medical datasets have a class imbalance, the authors addressed it by applying class distribution normalized Focal Loss described by  [Lin et al.(2017)](https://arxiv.org/pdf/1708.02002)"
      ],
      "metadata": {
        "id": "HvzDFqpXdQuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define Focal-Loss\n",
        "\"\"\"\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The focal loss for fighting against class-imbalance\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
        "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        logits & target should be tensors with shape [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        probs = torch.sigmoid(logits)\n",
        "        one_subtract_probs = 1.0 - probs\n",
        "        # add epsilon\n",
        "        probs_new = probs + self.epsilon\n",
        "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
        "        # calculate focal loss\n",
        "\n",
        "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
        "        pt = torch.exp(log_pt)\n",
        "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
        "        focal_loss = focal_loss * self.cls_weights\n",
        "        return torch.mean(focal_loss)"
      ],
      "metadata": {
        "id": "rYBbPbBN0K3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define F1 score metric\n",
        "\"\"\"\n",
        "class MyF1Score(Metric):\n",
        "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
        "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
        "        self.cfg = cfg\n",
        "        self.threshold = threshold\n",
        "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "\n",
        "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
        "        assert preds.shape == target.shape\n",
        "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
        "        target_str_batch = self.num_to_str(target)\n",
        "        tp, fp, fn = 0, 0, 0\n",
        "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
        "            for pred_str in pred_str_list:\n",
        "                if pred_str in target_str_list:\n",
        "                    tp += 1\n",
        "                if pred_str not in target_str_list:\n",
        "                    fp += 1\n",
        "\n",
        "            for target_str in target_str_list:\n",
        "                if target_str not in pred_str_list:\n",
        "                    fn += 1\n",
        "        self.tp += tp\n",
        "        self.fp += fp\n",
        "        self.fn += fn\n",
        "\n",
        "    def compute(self):\n",
        "        #To switch between F1 score and recall.\n",
        "        f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
        "        #rec = self.tp/(self.tp + self.fn)\n",
        "        return f1\n",
        "\n",
        "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
        "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
        "        batch_str_list = []\n",
        "        for one_sample_bool in batch_bool_list:\n",
        "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
        "            batch_str_list.append(lb_str_list)\n",
        "        return batch_str_list"
      ],
      "metadata": {
        "id": "gZoBvhY20QPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer:** Stochastic weigh averaging (SWA) with Adam optimizer and a learning rate of 1e-3 was used. More details about SWA in [Stochastic Weight Averaging](https://arxiv.org/abs/1803.05407)"
      ],
      "metadata": {
        "id": "AOpq34wIYmlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_cnn = SWA(torch.optim.Adam(model_cnn.parameters(), lr= 1e-3))\n",
        "optimizer_vit = SWA(torch.optim.Adam(model_vit.parameters(), lr= 1e-3))\n",
        "scheduler_cnn = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn,\n",
        "                                                                    T_max=16,\n",
        "                                                                    eta_min=1e-6)\n",
        "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit,\n",
        "                                                                    T_max=16,\n",
        "                                                                    eta_min=1e-6)\n",
        "\n",
        "criterion_vit = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "criterion_cnn = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)"
      ],
      "metadata": {
        "id": "46VUt-iL0a8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Training"
      ],
      "metadata": {
        "id": "aQOiPC85eZvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computational Requirement:**\n",
        "\n",
        "Original Paper: The authors of original paper ran all the experiments on single NVIDIA RTX8000 GPU with 48GB video memory, 2 CPU cores, and 64 GB system RAM. They used the internal clusters for their need.\n",
        "\n",
        "What we used: We used Google Cloud Platform single T4 GPU (3.75 GB), 16 CPU Core and 60 GB RAM. We couldn't access to multiple GPU units or high performance GPU virtual machines in Google Cloud Platform with the free credit. So there will be performance and calculation difference from the original paper to what we reproduced.\n"
      ],
      "metadata": {
        "id": "72caR6fkeeEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "********  **Reduced Label Learning:**  With large set of data, there was computational limitations. So in the draft notebook here, we have used reduced set of labels.\n",
        "\n",
        "******** We performed training with 100% of labels as well and the trained model file was downloaded here as well. Change the x value in the below code to change the learning label %.\n"
      ],
      "metadata": {
        "id": "Tw0BGiy9Zju7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Runtime:**\n",
        "\n",
        "|Reduced/Full|Epoch|Runtime|\n",
        "| --- | --- | --- |\n",
        "|100%|100|22 H 12 M|\n",
        "|10%|100|2 H 17 M|\n",
        "|10%|20|23 M 24 S|\n",
        "\n"
      ],
      "metadata": {
        "id": "2fP0NJfE5UeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(77)\n",
        "x=0.1 #currently set to use 10% of the labels for reduced label training\n",
        "onep=random.sample(range(0, len(train_image_list)), int(len(train_image_list)*x))\n",
        "all_train_image_list = [train_image_list[idx] for idx in onep]\n",
        "all_train_label_list = [train_label_list[idx] for idx in onep]"
      ],
      "metadata": {
        "id": "toWJOBZE0jW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(CFG, all_train_image_list, all_train_label_list, train_transform)\n",
        "valid_dataset = Dataset(CFG, valid_image_list, all_valid_label_list, valid_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)"
      ],
      "metadata": {
        "id": "7a7YO9Jv1gRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train SSL\n",
        "print('Training CASS-1Epoch for Draft')\n",
        "\n",
        "#enabled the model here in draft with just 1 epoch to show that the code is running, but we trained the model in GCP using 20 epoch & 100 epoch\n",
        "ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs=1)\n",
        "\n",
        "# Ran this in GCP with 20 epochs and 100 epochs\n",
        "#ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs=100)\n",
        "\n",
        "#Saving SSL Models\n",
        "print('Saving CASS-1Epoch for Draft')\n",
        "torch.save(model_cnn,'/content/drive/My Drive/DLH_CASS/BrainMRI/cass-mri-draft.pt') # save the model in your work directory\n",
        "torch.save(model_vit,'/content/drive/My Drive/DLH_CASS/BrainMRI/cass-mri-vit-draft.pt') # save the model in your work directory"
      ],
      "metadata": {
        "id": "E9B-91B81mZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the model from your work directory\n",
        "model_cnn=torch.load('/content/drive/My Drive/DLH_CASS/BrainMRI/cass-mri-draft.pt')\n",
        "model_vit=torch.load('/content/drive/My Drive/DLH_CASS/BrainMRI/cass-mri-vit-draft.pt')"
      ],
      "metadata": {
        "id": "D4hEiEIo16q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised fine-tuning:**\n",
        "\n",
        "**Hyperparams:** For supervised fine-tuning, the  Adam optimizer with a cosine annealing learning rate starting at 3e-04 was used. Since almost all medical datasets have some class imbalance, class distribution normalized Focal Loss was applied to navigate class imbalance. We trained with 50 epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "w19lkouvbOp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Fine tuning CASS-CNN-T')\n",
        "model_cnn.fc=nn.Linear(in_features=2048, out_features=4, bias=True)\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model_cnn.train()\n",
        "from torch.autograd import Variable\n",
        "best=0\n",
        "best_val=0\n",
        "last_loss=math.inf\n",
        "writer = SummaryWriter()\n",
        "#for epoch in range(50):\n",
        "for epoch in range(2): #setting the epoch to 2 for draft running\n",
        "    for images,label in train_loader:\n",
        "        model_cnn.train()\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model_cnn.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred_ts=model_cnn(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    train_score=metric.compute()\n",
        "    logs = {'train_loss': loss, 'f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "    writer.add_scalar(\"Supervised-CNN Loss/train\", loss, epoch)\n",
        "    writer.add_scalar(\"Supervised-CNN Recall/train\", train_score, epoch)\n",
        "    for name, weight in model_cnn.named_parameters():\n",
        "        writer.add_histogram(name,weight, epoch)\n",
        "        writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
        "    print(logs)\n",
        "    if best < train_score:\n",
        "        with torch.no_grad():\n",
        "            best=train_score\n",
        "            model_cnn.eval()\n",
        "            total_loss = 0\n",
        "            for images,label in valid_loader:\n",
        "                images = images.to(device)\n",
        "                label = label.to(device)\n",
        "                model_cnn.to(device)\n",
        "                pred_ts=model_cnn(images)\n",
        "                score_val = val_metric(pred_ts,label)\n",
        "                val_loss = criterion(pred_ts, label)\n",
        "                total_loss += val_loss.detach()\n",
        "            avg_loss=total_loss/ len(train_loader)\n",
        "            print('Val Loss:',avg_loss)\n",
        "            val_score=val_metric.compute()\n",
        "            print('CNN Validation Score:',val_score)\n",
        "            writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
        "            if avg_loss > last_loss:\n",
        "                counter+=1\n",
        "            else:\n",
        "                counter=0\n",
        "\n",
        "            last_loss = avg_loss\n",
        "            if counter > 5:\n",
        "                print('Early Stopping!')\n",
        "                break\n",
        "            else:\n",
        "                if val_score > best_val:\n",
        "                    best_val=val_score\n",
        "                    print('Saving')\n",
        "                    torch.save(model_cnn,'/content/drive/My Drive/DLH_CASS/BrainMRI/CASS-Draft-CNN-part-ft.pt') #save in your work directory\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "pV8wqcS617kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Fine tunning CASS-ViT')\n",
        "model_vit.head=nn.Linear(in_features=768, out_features=4, bias=True)\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model_vit.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model_vit.train()\n",
        "val_metric=MyF1Score(cfg)\n",
        "writer = SummaryWriter()\n",
        "from torch.autograd import Variable\n",
        "best=0\n",
        "best_val=0\n",
        "last_loss=math.inf\n",
        "#for epoch in range(50):\n",
        "for epoch in range(2): #setting the epoch to 2 for draft running\n",
        "    for images,label in train_loader:\n",
        "        model_vit.train()\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model_vit.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred_ts=model_vit(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts,label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    train_score=metric.compute()\n",
        "    logs = {'train_loss': loss, 'f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "    writer.add_scalar(\"Supervised-ViT Loss/train\", loss, epoch)\n",
        "    writer.add_scalar(\"Supervised-ViT Recall/train\", train_score, epoch)\n",
        "    for name, weight in model_vit.named_parameters():\n",
        "        writer.add_histogram(name,weight, epoch)\n",
        "        writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
        "    print(logs)\n",
        "    if best < train_score:\n",
        "        with torch.no_grad():\n",
        "            best=train_score\n",
        "            model_vit.eval()\n",
        "            total_loss = 0\n",
        "            for images,label in valid_loader:\n",
        "                images = images.to(device)\n",
        "                label = label.to(device)\n",
        "                model_vit.to(device)\n",
        "                pred_ts=model_vit(images)\n",
        "                score_val = val_metric(pred_ts,label)\n",
        "                val_loss = criterion(pred_ts, label)\n",
        "                total_loss += val_loss.detach()\n",
        "            avg_loss=total_loss/ len(train_loader)\n",
        "            print('Val Loss:',avg_loss)\n",
        "            val_score=val_metric.compute()\n",
        "            print('ViT Validation Score:',val_score)\n",
        "            writer.add_scalar(\"ViT Supervised F1/Validation\", val_score, epoch)\n",
        "            if avg_loss > last_loss:\n",
        "                counter+=1\n",
        "            else:\n",
        "                counter=0\n",
        "\n",
        "            last_loss = avg_loss\n",
        "            if counter > 5:\n",
        "                print('Early Stopping!')\n",
        "                break\n",
        "            else:\n",
        "                if val_score > best_val:\n",
        "                    best_val=val_score\n",
        "                    print('Saving')\n",
        "                    torch.save(model_cnn,\n",
        "                        '/content/drive/My Drive/DLH_CASS/BrainMRI/CASS-Draft-ViT-part-ft.pt') #save in your work directory\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "-BFyrzm01-n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "DjiRu0ohhgbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics Description:**\n",
        "\n",
        "we used the F1 score as our metric, which is defined as\n",
        "\n",
        "\n",
        "F1 = (2* Precision* Recall) / Precision+Recall = (2* TP) / (2* TP+FP+FN)"
      ],
      "metadata": {
        "id": "GgYUEgPgd-y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Dataset(CFG, test_image_list,all_test_label_list, test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, drop_last=True)"
      ],
      "metadata": {
        "id": "sAzGzcnD88a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('/content/drive/My Drive/DLH_CASS/BrainMRI/CASS-Draft-CNN-part-ft.pt')"
      ],
      "metadata": {
        "id": "UD1p8JWx89sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "8TeAZYOh9AaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('/content/drive/My Drive/DLH_CASS/BrainMRI/CASS-Draft-ViT-part-ft.pt')"
      ],
      "metadata": {
        "id": "oRtyGks-v00f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "bVIDiA7rv0g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "CASS model is more robust to changes in batch size and pretraining epochs. Authors from original paper captured the results by training the data with 1%, 10% and 100% labelled data and 100 epochs. They compared the result with other state of art models to show the efficiency of CASS.\n",
        "\n",
        "We performed training with 10% labelled data with 100 epochs, 20 epochs for self supervised training and 50 epoch & 10 epoch for fine tuning.  We loaded those models below and calculated the f1 score for testing.\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result Metrics:**\n",
        "\n",
        "Metrics for batch size 16 for 10% reduced label learning with 100 epoch for BrainMRI classification dataset.\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1wGB1VrTJOk1yjJ-WvNMmVbCA4t6IxVLV)"
      ],
      "metadata": {
        "id": "WiItquWFR4ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained model with 10% reduced label, trained for 20 epoch and fine tuned with 10 epoch - f1 score as 0.5440\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '1sQMmCZr5wzbxG0QrtiIhJPoTW_GnrdtY' --output CASSCNN10R20E10ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN10R20E10ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained model with 10% reduced label and pretrained for 100 epoch and fine tuned with 50 epoch and got the f1 score as 0.6920\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '1ywhZ2FhKW3IGsgUdFuIvOOHHOsOIrCm2' --output CASSCNN10R100E50ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN10R100E50ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "2UAQsOAh06mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " *******We also trained the model with 100% data and 100 epoch and noticed vanishing gradient problem.\n",
        "\n",
        " 100% data with 100 epoch runs for 22 hours in single core T4 GPU(3.5GB Memory) 16 Core CPU and 60 GB RAM\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "71BUHQN92Fq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runtime Comparison:\n",
        "\n",
        "This is the Self-supervised pretraining time comparison between DINO ( [Caron et al. (2021)](https://arxiv.org/abs/2104.14294)), the Original CASS implementation by the authors for 100 epochs on a single RTX8000 GPU and the reproduced time with GCP NVIDIA T4 - 1 GPU\n",
        "\n",
        "| DataSet | DINO | CASS - Original | CASS - Reproduced |\n",
        "| --- | --- | --- | --- |\n",
        "| BrainMRI | 26 H 21 M | 7 H 11 M | 22 H 12 M\n",
        "\n",
        "Even with limited computational resources, we could see that the training time is lesser than the state of art DINO model that the original authors compared with."
      ],
      "metadata": {
        "id": "dnh88fw63dYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics Comparison:\n",
        "\n",
        "Original paper has compared the CASS result with other state of art model techniques such as DINO and transfer. To achieve this result they pretrained and fine tuned the model with 100 epoch.\n",
        "\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1mDix1rSuNpTFtRo_2ekmM0MAQEp1Sfps)\n"
      ],
      "metadata": {
        "id": "Hz84HMA1iJ89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Test Result:\n",
        "\n",
        "|Techniques|Backbone|F1 Score 1%|F1Score 10%|\n",
        "| --- | --- | --- | --- |\n",
        "|CASS|Resnet-50|0.3968|0.6920|\n",
        "|CASS|ViT B/16|0.2318|0.6164|\n",
        "\n",
        "We couldn't achieve the exact score from the original authors because of the computational limits, but with our limited resources we could get a good score that is comparable with the other models."
      ],
      "metadata": {
        "id": "nQHZBwGN8Oyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablations"
      ],
      "metadata": {
        "id": "nJHjkF4EBggR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Change in epochs:\n",
        "\n",
        "\n",
        "Study the effect of longer pre training on the selected dataset by changing the epoch and observe the performance variations.\n",
        "\n",
        "Performance Comparison over a varied number of epochs for 10% label training:\n",
        "\n",
        "|Epoch| F1-Score|\n",
        "|---|---|\n",
        "|20|0.5440|\n",
        "|50|0.6426|\n",
        "|100|0.6920|\n",
        "\n"
      ],
      "metadata": {
        "id": "-mJwUkwEBcFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We trained a model with 10% reduced label and pretrained for 100 epoch and fine tuned with 50 epoch and got the f1 score as 0.6920\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '1ywhZ2FhKW3IGsgUdFuIvOOHHOsOIrCm2' --output CASSCNN10R100E50ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN10R100E50ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained model with 10% reduced label, trained for 20 epoch and fine tuned with 10 epoch - f1 score as 0.5440\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '1sQMmCZr5wzbxG0QrtiIhJPoTW_GnrdtY' --output CASSCNN10R20E10ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN10R20E10ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "sUpz9RstL4UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change in Batch Size:\n",
        "\n",
        "Trained the model with different batch sizes and the model is robust to changes.\n",
        "\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1hdikyfvPxAahWEPgwJaeugw6ntWV4DBD)\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1v4XENTuyg8rbvAQBKvK1nFaiMLeLmrFW)\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1PNSbofP-jy_UMEE90oEzUlrNauuWfGxU)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Pk1MJpgCSOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with 10% reduced label and pretrained for 100 epoch and fine tuned with 50 epoch for batch size 16\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '1ywhZ2FhKW3IGsgUdFuIvOOHHOsOIrCm2' --output CASSCNN10R100E50ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN10R100E50ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "3CaONFv9M8mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with 10% reduced label and pretrained for 100 epoch and fine tuned with 50 epoch for batch size 8\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '17e_fF0nQc2YT0sJh-UFjHHI-ehENreLN' --output CASSCNN8BAT10R100E50ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN8BAT10R100E50ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "OmCT4nZ4NIF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with 10% reduced label and pretrained for 100 epoch and fine tuned with 50 epoch for batch size 4\n",
        "# loaded that pretrained model here\n",
        "!gdown --id '1K2hgb4bXkI7sppGru4hs3w5OQ8-2JBLy' --output CASSCNN4BAT10R100E50ft.pt\n",
        "\n",
        "model = torch.load('/content/CASSCNN4BAT10R100E50ft.pt')\n",
        "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
        "metric = MyF1Score(cfg)\n",
        "val_metric=MyF1Score(cfg)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images,label in test_loader:\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        model.to(device)\n",
        "        pred_ts=model(images)\n",
        "        loss = criterion(pred_ts, label)\n",
        "        score = metric(pred_ts, label)\n",
        "test_score=metric.compute()\n",
        "#logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "logs = {'train_loss': loss, 'f1': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
        "print(logs)"
      ],
      "metadata": {
        "id": "xN3igfsDWdwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The code given in github repository is reproducible with few changes from our side for data preprocessing and some code fixes for the error we encountered during execution. Due to hardware limitations, we couldn't acheive the Self-supervised pretraining time mentioned by the authors in the paper. Also while training with 100 epoch, we noticed the vanishing gradient issue.\n",
        "\n",
        "![picture](https://drive.google.com/uc?/export=view&id=1o5JrUp7w-n8TxBWa8q28OU6hieuYY7u-)\n",
        "\n",
        "For Ablations, we trained the model with different epochs and different batch sizes. With these studies, we can see that CASS is more robust to changes as mentioned by the authors.\n",
        "\n",
        "For others who are trying to reproduce this, if you need to achieve the exact result as the authors, you need to have the computational resources available. But we can execute the code with reduced epoch and small subset of data using GCP free trial version or Google Colab Pro. Also the code has the data processing steps only for MedMNIST and ISIC 2019. If you are using other dataset, you need to preprocess the code like how we did for BrainMRI Classification. There are few errors with optimizer and other logic in e2e fine tuning code, which needs to be fixed. We have the defect fixed running code in this notebook.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fk43_CMMjOqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Pranav Singh & Jacopo Cirrone, [Efficient Representation Learning for Healthcare with\n",
        "Cross-Architectural Self-Supervision], [Proceedings of Machine Learning Research 219:1–36, 2023],[https://proceedings.mlr.press/v219/singh23a/singh23a.pdf]\n",
        "\n",
        "2. Original Source Code: https://github.com/pranavsinghps1/CASS\n",
        "\n",
        "3. Kaggle Source Data Link for BrainMRI: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
        "\n",
        "4. Kaggle Source Data Link for ISIC2019: https://www.kaggle.com/datasets/andrewmvd/isic-2019\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}