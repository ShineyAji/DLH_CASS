{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf5e5319-e9c2-4e47-a871-9dcd63d7f737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timm\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tsfm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8475083-9db9-4441-a57d-412c6483429e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\n",
      "Requirement already satisfied: torchcontrib in /opt/conda/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.25.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.10.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.11.2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.22.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.2.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch_lightning) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch_lightning timm torchcontrib tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d39df3db-4d7c-4ba5-b1ff-7b20d00b52f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_path = '/home/jupyter/scratch/ISIC2019/train/ISIC_2019_Training_GroundTruth.csv'\n",
    "    train_imgs_dir = '/home/jupyter/scratch/ISIC2019/train/ISIC_2019_Training_Input/ISIC_2019_Training_Input'\n",
    "    # model info\n",
    "    # label info\n",
    "    label_num2str = {0: 'NV',\n",
    "                     1: 'MEL',\n",
    "                     2: 'BCC',\n",
    "                     3: 'BKL',\n",
    "                     4: 'AK',\n",
    "                     5: 'SCC',\n",
    "                     6: 'VASC',\n",
    "                     7: 'DF'\n",
    "                     }\n",
    "    label_str2num = {'NV': 0,\n",
    "                     'MEL':1,\n",
    "                     'BCC':2,\n",
    "                     'BKL':3,\n",
    "                     'AK':4,\n",
    "                     'SCC':5,\n",
    "                     'VASC':6,\n",
    "                     'DF':7 \n",
    "                     }\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight = [1.0, 0.4717294571343815, 0.39523385741125283, 0.3535449421536636, 0.2405023237417186, 0.2242064669237615, 0.20134480371798677, 0.2]\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='vit_base_patch16_384'\n",
    "    seed = 77\n",
    "    num_classes = 8\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    accum_grad_batch = 1\n",
    "    early_stop_delta = 1e-7\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b79645d-7003-4ea5-af6a-ee3d6eca5cff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 77\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_everything(77)\n",
    "cfg=CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a87c7d9-88d6-4995-9f51-44df64ce12be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define train & valid image transformation\n",
    "\"\"\"\n",
    "DATASET_IMAGE_MEAN = (0.485, 0.456, 0.406)\n",
    "DATASET_IMAGE_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomPerspective(distortion_scale=0.2),], p=0.3),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomAffine(degrees=10),], p=0.3),\n",
    "                                tsfm.RandomVerticalFlip(p=0.3),\n",
    "                                tsfm.RandomHorizontalFlip(p=0.3),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
    "\n",
    "valid_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
    "\n",
    "test_transform = tsfm.Compose([tsfm.Resize((384, 384)),\n",
    "                               tsfm.ToTensor(),\n",
    "                               tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "395764c4-7b62-48f3-b46b-877a62b5b52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define dataset class\n",
    "\"\"\"\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, cfg, img_names: list, labels: list, transform=None):\n",
    "        self.img_dir = cfg.train_imgs_dir\n",
    "        self.img_names = img_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx]+'.jpg')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_ts = self.transform(img)\n",
    "        label_ts = self.labels[idx]\n",
    "        return img_ts, label_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "625dbcf0-cc7e-4476-8481-c2424246f487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Focal-Loss\n",
    "\"\"\"\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "978f6518-f922-4dc7-a4a3-287206e61f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define F1 score metric\n",
    "\"\"\"\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        #To switch between F1 score and recall.\n",
    "        #f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        rec = self.tp/(self.tp + self.fn)\n",
    "        return rec\n",
    "        #return f1\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "583128c2-8ad8-4ca0-82f7-e6d91cb256c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(cfg.data_path)\n",
    "df['diagnosis'] = df[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK']].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d143eaa5-41db-43e4-98ca-9facec510cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(df['image'], df['diagnosis'], test_size=0.2, random_state=77)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['image'], df['diagnosis'], test_size=0.2, random_state=77)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe513829-59d5-4194-ab21-1f80c1759b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_img_names: list = X_train.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ccd226c-d064-4fd2-93bd-712565dcb792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_img_names_valid: list = X_val.values.tolist()\n",
    "\n",
    "all_img_names_test: list = X_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3820236-4d7a-4265-84e0-b7e9b5e78516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5067"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_img_names)\n",
    "len(all_img_names_valid)\n",
    "len(all_img_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "658d2aed-2711-44a3-8c7f-7871163d5a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15198, 8])\n"
     ]
    }
   ],
   "source": [
    "all_img_labels_ts = []\n",
    "\n",
    "\n",
    "\n",
    "for tmp_num in y_train:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    label=CFG.label_num2str.get(tmp_num)\n",
    "    if label is not None:\n",
    "        k=int(tmp_num)\n",
    "        tmp_label[k] = 1.0\n",
    "        all_img_labels_ts.append(tmp_label) \n",
    "# Convert the list of tensors to a single tensor\n",
    "all_img_labels_ts = torch.stack(all_img_labels_ts)\n",
    "\n",
    "# Display the shape of the resulting tensor\n",
    "print(all_img_labels_ts.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c5fc24a-13e1-407c-afea-f6a3dad721cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_labels_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27785d6e-d4ea-42b4-a8ba-3b8b3b565c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_img_labels_val_ts = []\n",
    "\n",
    "for tmp_num in y_val:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    label=CFG.label_num2str.get(tmp_num)\n",
    "    k=int(tmp_num)\n",
    "    tmp_label[k] = 1.0\n",
    "    all_img_labels_val_ts.append(tmp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e6bf7e51-a8e8-47f5-b691-f81ffcd96570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_img_labels_test_ts = []\n",
    "\n",
    "for tmp_num in y_test:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    label=CFG.label_num2str.get(tmp_num)\n",
    "    k=int(tmp_num)\n",
    "    tmp_label[k] = 1.0\n",
    "    all_img_labels_test_ts.append(tmp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "784b0a6f-d3cb-4908-b37a-6158b3706674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
    "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
    "model_cnn.to(device)\n",
    "model_vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "705be090-22e5-42d1-b00d-d45fc1766b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs):\n",
    "    writer = SummaryWriter()\n",
    "    phase = 'train'\n",
    "    model_cnn.train()\n",
    "    model_vit.train()\n",
    "    f1_score_cnn=0\n",
    "    f1_score_vit=0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            for img,_ in train_loader:\n",
    "                f1_score_cnn=0\n",
    "                f1_score_vit=0\n",
    "                img = img.to(device)\n",
    "                pred_vit = model_vit(img)\n",
    "                pred_cnn = model_cnn(img)\n",
    "                model_sim_loss=loss_fn(pred_vit,pred_cnn)\n",
    "                loss = model_sim_loss.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cnn.step()\n",
    "                optimizer_vit.step()\n",
    "                scheduler_cnn.step()\n",
    "                scheduler_vit.step()\n",
    "            print('For -',i,'Loss:',loss) \n",
    "            writer.add_scalar(\"Self-Supervised Loss/train\", loss, i)\n",
    "    writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e776821-9f77-4ecb-b89e-c29f28f9981a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_cnn = SWA(torch.optim.Adam(model_cnn.parameters(), lr= 1e-3))\n",
    "optimizer_vit = SWA(torch.optim.Adam(model_vit.parameters(), lr= 1e-3))\n",
    "scheduler_cnn = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "\n",
    "criterion_vit = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "criterion_cnn = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "486492ef-6d77-4ee9-9398-5feef3ffa178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    x =  torch.nn.functional.normalize(x, dim=-1, p=2)\n",
    "    y =  torch.nn.functional.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8331770a-d601-4bde-badf-b96327927b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(77)\n",
    "x=0.1 #currently set to use 10% of the labels for reduced label training \n",
    "onep=random.sample(range(0, len(X_train)), int(len(X_train)*x))\n",
    "all_img_names_train = [all_img_names[idx] for idx in onep]\n",
    "all_img_labels_ts_train = [all_img_labels_ts[idx] for idx in onep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ebd880c-ef78-4ffb-8efb-90269cbefa30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(CFG, all_img_names_train,all_img_labels_ts_train, train_transform)\n",
    "valid_dataset = Dataset(CFG, all_img_names_valid, all_img_labels_val_ts, valid_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "test_dataset = Dataset(CFG, all_img_names_test, all_img_labels_test_ts, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba06ee1b-a9f7-48a3-96bb-23de9aff54f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5066"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e10bd25c-82f6-4955-882f-91fba5511bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1006f52c-024f-479e-9747-a69a2b0e1900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cov-T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 0 Loss: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:19<00:00, 199.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Saving Cov-T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Train SSL\n",
    "print('Training Cov-T')\n",
    "ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs=1)\n",
    "#Saving SSL Models\n",
    "print('Saving Cov-T')\n",
    "torch.save(model_cnn,'./cass-r50-isic.pt')\n",
    "torch.save(model_vit,'./cass-r50-vit-isic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3e642306-e8a4-4234-92be-7d6d0f246037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cnn=torch.load('./cass-r50-isic.pt')\n",
    "model_vit=torch.load('./cass-r50-vit-isic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13154fde-a607-4456-85de-f7fea1177ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tunning Cov-T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>), 'Recall': tensor(1.), 'lr': 0.0002971273994202864}\n",
      "Val Loss: tensor(0.0007, device='cuda:0')\n",
      "CNN Validation Score: tensor(1.)\n",
      "Saving\n",
      "{'train_loss': tensor(3.7421e-05, device='cuda:0', grad_fn=<MeanBackward0>), 'Recall': tensor(1.), 'lr': 0.00028861999011043244}\n"
     ]
    }
   ],
   "source": [
    "#Train Correspong Supervised CNN\n",
    "print('Fine tunning Cov-T')\n",
    "model_cnn.fc=nn.Linear(in_features=2048, out_features=8, bias=True)\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "val_metric=MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model_cnn.train()\n",
    "from torch.autograd import Variable\n",
    "best=0\n",
    "best_val=0\n",
    "last_loss=math.inf\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    for images,label in train_loader:\n",
    "        model_cnn.train()\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model_cnn.to(device)\n",
    "        pred_ts=model_cnn(images)\n",
    "\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "    train_score=metric.compute()\n",
    "    logs = {'train_loss': loss, 'Recall': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "    writer.add_scalar(\"Supervised-CNN Loss/train\", loss, epoch)\n",
    "    writer.add_scalar(\"Supervised-CNN Recall/train\", train_score, epoch)\n",
    "\n",
    "    for name, weight in model_cnn.named_parameters():\n",
    "        #print( weight,epoch)\n",
    "        writer.add_histogram( name, weight,epoch)\n",
    "        #writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "    print(logs)\n",
    "    if best < train_score:\n",
    "        with torch.no_grad():\n",
    "            best=train_score\n",
    "            model_cnn.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_cnn.to(device)\n",
    "                pred_ts=model_cnn(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)   \n",
    "            print('Val Loss:',avg_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('CNN Validation Score:',val_score)\n",
    "            writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "\n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_cnn,\n",
    "                        './CASS-CNN-part-ft.pt')\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "febd7c59-3793-4901-8952-4c649e9050da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(2.5050e-06, device='cuda:0', grad_fn=<MeanBackward0>), 'Recall': tensor(0.9684), 'lr': 0.0002971273994202864}\n",
      "Val Loss: tensor(0.0001, device='cuda:0')\n",
      "CNN Validation Score: tensor(1.)\n",
      "Saving\n",
      "{'train_loss': tensor(2.0164e-06, device='cuda:0', grad_fn=<MeanBackward0>), 'Recall': tensor(0.9842), 'lr': 0.00028861999011043244}\n",
      "Val Loss: tensor(0.0001, device='cuda:0')\n",
      "CNN Validation Score: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "model_vit=torch.load('./cass-r50-vit-isic.pt')\n",
    "model_vit.head=nn.Linear(in_features=768, out_features=8, bias=True)\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model_vit.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model_vit.train()\n",
    "val_metric=MyF1Score(cfg)\n",
    "writer = SummaryWriter()\n",
    "from torch.autograd import Variable\n",
    "best=0\n",
    "best_val=0\n",
    "last_loss=math.inf\n",
    "for epoch in range(2):\n",
    "    for images,label in train_loader:\n",
    "        model_vit.train()\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model_vit.to(device)\n",
    "        pred_ts=model_vit(images)\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "    train_score=metric.compute()\n",
    "    logs = {'train_loss': loss, 'Recall': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "    writer.add_scalar(\"Supervised-ViT Loss/train\", loss, epoch)\n",
    "    writer.add_scalar(\"Supervised-ViT Recall/train\", train_score, epoch)\n",
    "    for name, weight in model_vit.named_parameters():\n",
    "        writer.add_histogram(name,weight, epoch)\n",
    "        #writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "    print(logs)\n",
    "    if best < train_score:\n",
    "        with torch.no_grad():\n",
    "            best=train_score\n",
    "            model_cnn.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_cnn.to(device)\n",
    "                pred_ts=model_cnn(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)   \n",
    "            print('Val Loss:',avg_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('CNN Validation Score:',val_score)\n",
    "            writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "\n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_cnn,\n",
    "                        './CASS-ViT-part-ft.pt')\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12a2e300-88c4-41cf-97a3-139a69567fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(0.0002, device='cuda:0'), 'Recall': tensor(1.), 'lr': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "model=torch.load('./CASS-CNN-part-ft.pt')\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "val_metric=MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model_cnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images,label in test_loader:\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model.to(device)\n",
    "        pred_ts=model(images)\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts, label)\n",
    "test_score=metric.compute()\n",
    "logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "048a6601-a00e-4bc8-936b-268843c5f783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(3.3413e-05, device='cuda:0'), 'Recall': tensor(1.), 'lr': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "model=torch.load('./CASS-ViT-part-ft.pt')\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "val_metric=MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model_cnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images,label in test_loader:\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model.to(device)\n",
    "        pred_ts=model(images)\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts, label)\n",
    "test_score=metric.compute()\n",
    "logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd381570-d6ba-4027-bb27-2b651e52fdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
